# Analyse D√©taill√©e - Exercice 4 : ELK Stack (Elasticsearch, Logstash, Kibana, Filebeat)

## Vue d'Ensemble

**Objectif** : D√©boguer une stack ELK compl√®te pour la collecte, le traitement et la visualisation de logs.

**Complexit√©** : Niveau Avanc√© ‚≠ê‚≠ê‚≠ê‚≠ê

**Services** : 
- Elasticsearch 8.11.0 (moteur de recherche et stockage)
- Logstash 8.11.0 (traitement et enrichissement des logs)
- Kibana 8.11.0 (visualisation et dashboards)
- Filebeat 8.11.0 (collecteur de logs l√©gers)

**Bugs Identifi√©s** : 14 probl√®mes critiques et de performance

---

## üêõ Bug #1 : Version Docker Compose Obsol√®te

### Sympt√¥mes
```yaml
version: '3.8'
```
- Warning lors de `docker compose up`
- Syntaxe d√©pr√©ci√©e depuis Docker Compose v2

### Diagnostic
La directive `version` n'est plus n√©cessaire et g√©n√®re des avertissements inutiles.

### Solution
**SUPPRIMER** compl√®tement la ligne `version: '3.8'`

### Impact
- ‚úÖ Pas de warnings
- ‚úÖ Code moderne et propre

---

## üêõ Bug #2 : Absence de R√©seau D√©di√©

### Sympt√¥mes
```yaml
services:
  elasticsearch:
    # Pas de configuration r√©seau
  logstash:
    # Pas de configuration r√©seau
  # etc...
```
- Services sur le r√©seau bridge par d√©faut
- Pas d'isolation r√©seau
- Communication non s√©curis√©e

### Diagnostic
Sans r√©seau personnalis√©, la stack ELK n'est pas isol√©e des autres conteneurs sur l'h√¥te.

### Solution
```yaml
networks:
  elk-network:
    driver: bridge

services:
  elasticsearch:
    networks:
      - elk-network
  logstash:
    networks:
      - elk-network
  kibana:
    networks:
      - elk-network
  filebeat:
    networks:
      - elk-network
```

### Impact
- ‚úÖ Isolation r√©seau compl√®te
- ‚úÖ Communication s√©curis√©e entre services ELK
- ‚úÖ Pas d'acc√®s externe non autoris√©

---

## üêõ Bug #3 : Pas de Health Checks

### Sympt√¥mes
```yaml
elasticsearch:
  image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
  # Pas de healthcheck

logstash:
  # Pas de healthcheck

kibana:
  # Pas de healthcheck
```
- Services d√©marrent dans le d√©sordre
- Kibana/Logstash tentent de se connecter avant qu'Elasticsearch soit pr√™t
- Erreurs de connexion massives au d√©marrage

### Diagnostic
Elasticsearch met ~60s √† d√©marrer, Kibana ~90s. Sans health checks, impossible de garantir un d√©marrage orchestr√©.

### Solution

#### Elasticsearch Health Check
```yaml
elasticsearch:
  healthcheck:
    test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 60s
```
- Endpoint `/_cluster/health` : Status du cluster ES
- `start_period: 60s` : Temps d'initialisation n√©cessaire

#### Logstash Health Check
```yaml
logstash:
  healthcheck:
    test: ["CMD-SHELL", "curl -f http://localhost:9600/_node/stats || exit 1"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 60s
```
- Endpoint `/_node/stats` : Monitoring API Logstash
- Port 9600 : API de monitoring

#### Kibana Health Check
```yaml
kibana:
  healthcheck:
    test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 90s
```
- Endpoint `/api/status` : Status API Kibana
- `start_period: 90s` : Kibana est lent au d√©marrage

### Impact
- ‚úÖ D√©marrage fiable et orchestr√©
- ‚úÖ Pas d'erreurs de connexion
- ‚úÖ Monitoring de sant√© des services

---

## üêõ Bug #4 : depends_on Simple Sans Condition

### Sympt√¥mes
```yaml
logstash:
  depends_on:
    - elasticsearch  # Simple dependency

kibana:
  depends_on:
    - elasticsearch  # Simple dependency

filebeat:
  depends_on:
    - elasticsearch
    - logstash
```
- Logstash/Kibana d√©marrent avant qu'Elasticsearch soit pr√™t
- Filebeat d√©marre avant que Logstash soit pr√™t
- **Erreurs massives** : `Connection refused [elasticsearch:9200]`

### Diagnostic
`depends_on` simple ne garantit que l'ordre de cr√©ation des conteneurs, pas leur √©tat "ready".

### Solution
```yaml
logstash:
  depends_on:
    elasticsearch:
      condition: service_healthy

kibana:
  depends_on:
    elasticsearch:
      condition: service_healthy

filebeat:
  depends_on:
    elasticsearch:
      condition: service_healthy
    logstash:
      condition: service_healthy
```

### Impact
- ‚úÖ Chaque service attend que ses d√©pendances soient healthy
- ‚úÖ D√©marrage s√©quentiel correct : ES ‚Üí Logstash/Kibana ‚Üí Filebeat
- ‚úÖ Pas de retry inutiles

---

## üêõ Bug #5 : Ports Hardcod√©s

### Sympt√¥mes
```yaml
elasticsearch:
  ports:
    - "9200:9200"  # Hardcod√©
    - "9300:9300"

logstash:
  ports:
    - "5044:5044"
    - "5000:5000/tcp"
    - "5000:5000/udp"

kibana:
  ports:
    - "5601:5601"
```
- Impossible de changer les ports sans √©diter le YAML
- Conflit potentiel si ports d√©j√† utilis√©s

### Diagnostic
Les ports d'exposition doivent √™tre configurables via variables d'environnement.

### Solution
```yaml
elasticsearch:
  ports:
    - "${ELASTICSEARCH_PORT}:9200"

logstash:
  ports:
    - "${LOGSTASH_BEATS_PORT}:5044"
    - "${LOGSTASH_TCP_PORT}:5000/tcp"
    - "${LOGSTASH_UDP_PORT}:5000/udp"

kibana:
  ports:
    - "${KIBANA_PORT}:5601"
```

Avec `.env` :
```bash
ELASTICSEARCH_PORT=9200
LOGSTASH_BEATS_PORT=5044
LOGSTASH_TCP_PORT=5000
LOGSTASH_UDP_PORT=5000
KIBANA_PORT=5601
```

### Impact
- ‚úÖ Configuration flexible
- ‚úÖ √âvite les conflits de ports
- ‚úÖ Multi-instances possibles

---

## üêõ Bug #6 : Variables M√©moire Hardcod√©es

### Sympt√¥mes
```yaml
elasticsearch:
  environment:
    - ES_JAVA_OPTS=-Xms512m -Xmx512m  # Hardcod√©

logstash:
  environment:
    - LS_JAVA_OPTS=-Xmx256m -Xms256m  # Hardcod√©
```
- Impossible d'ajuster la m√©moire selon l'environnement
- 512m peut √™tre insuffisant pour Elasticsearch en production

### Diagnostic
La m√©moire JVM doit √™tre configurable selon les ressources disponibles.

### Solution
```yaml
elasticsearch:
  environment:
    - ES_JAVA_OPTS=-Xms${ES_MEMORY} -Xmx${ES_MEMORY}

logstash:
  environment:
    - LS_JAVA_OPTS=-Xmx${LOGSTASH_MEMORY} -Xms${LOGSTASH_MEMORY}
```

Avec `.env` :
```bash
ES_MEMORY=1g
LOGSTASH_MEMORY=512m
```

### Recommandations M√©moire
- **Dev** : ES 512m-1g, Logstash 256m-512m
- **Prod** : ES 2g-4g+, Logstash 1g-2g

### Impact
- ‚úÖ M√©moire ajustable sans √©diter YAML
- ‚úÖ Optimisation par environnement

---

## üêõ Bug #7 : Pas de Container Names

### Sympt√¥mes
```yaml
elasticsearch:
  image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
  # Pas de container_name
```
- Noms auto-g√©n√©r√©s : `exercice-4-elk-elasticsearch-1`
- Difficile √† identifier dans `docker ps`

### Diagnostic
Les container names explicites facilitent l'administration et le debugging.

### Solution
```yaml
elasticsearch:
  container_name: elk-elasticsearch

logstash:
  container_name: elk-logstash

kibana:
  container_name: elk-kibana

filebeat:
  container_name: elk-filebeat
```

### Impact
- ‚úÖ Identification claire
- ‚úÖ Commandes docker plus simples
- ‚úÖ Logs facilement tra√ßables

---

## üêõ Bug #8 : Pas de Restart Policies

### Sympt√¥mes
```yaml
elasticsearch:
  # Pas de restart policy

logstash:
  # Pas de restart policy
```
- Services ne red√©marrent pas apr√®s un crash
- Pas de reprise apr√®s reboot serveur

### Diagnostic
En production, les services ELK doivent red√©marrer automatiquement.

### Solution
```yaml
elasticsearch:
  restart: unless-stopped

logstash:
  restart: unless-stopped

kibana:
  restart: unless-stopped

filebeat:
  restart: unless-stopped
```

### Impact
- ‚úÖ Haute disponibilit√©
- ‚úÖ Reprise automatique apr√®s incident

---

## üêõ Bug #9 : Pas d'ulimits pour Elasticsearch (CRITIQUE!)

### Sympt√¥mes
```yaml
elasticsearch:
  # Pas d'ulimits configur√©s
```
- **Erreur au d√©marrage** : `max virtual memory areas vm.max_map_count [65530] is too low`
- Elasticsearch refuse de d√©marrer ou crashe sous charge
- **Performance d√©grad√©e** avec trop de fichiers ouverts

### Diagnostic
Elasticsearch n√©cessite des limites syst√®me sp√©cifiques pour fonctionner correctement :
1. **memlock** : Verrouillage m√©moire pour √©viter le swap
2. **nofile** : Nombre de fichiers ouverts (indices, shards)

### Solution
```yaml
elasticsearch:
  ulimits:
    memlock:
      soft: -1
      hard: -1
    nofile:
      soft: 65536
      hard: 65536
```

### Explication
- `memlock: -1` : Illimit√© (permet le verrouillage m√©moire)
- `nofile: 65536` : 65k fichiers ouverts (suffisant pour gros clusters)

### Note Syst√®me
Sur l'h√¥te, il faut aussi :
```bash
sudo sysctl -w vm.max_map_count=262144
```

### Impact
- ‚úÖ Elasticsearch d√©marre correctement
- ‚úÖ Performance optimale
- ‚úÖ Pas de crash sous charge

---

## üêõ Bug #10 : Pas de bootstrap.memory_lock

### Sympt√¥mes
```yaml
elasticsearch:
  environment:
    - discovery.type=single-node
    - ES_JAVA_OPTS=-Xms512m -Xmx512m
    # Manque bootstrap.memory_lock=true
```
- M√©moire Elasticsearch peut √™tre swapp√©e sur disque
- **Performance catastrophique** si swap activ√©
- Latence des requ√™tes x100

### Diagnostic
Le swap est le cauchemar d'Elasticsearch. Il faut verrouiller la m√©moire en RAM.

### Solution
```yaml
elasticsearch:
  environment:
    - bootstrap.memory_lock=true
```

Combin√© avec :
```yaml
ulimits:
  memlock:
    soft: -1
    hard: -1
```

### Impact
- ‚úÖ M√©moire JVM verrouill√©e en RAM
- ‚úÖ Performance maximale
- ‚úÖ Latence pr√©visible

---

## üêõ Bug #11 : Volumes Read-Only Manquants

### Sympt√¥mes
```yaml
logstash:
  volumes:
    - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    - ./logstash/pipeline:/usr/share/logstash/pipeline
    # Pas de :ro (read-only)

filebeat:
  volumes:
    - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml
    - /var/log:/var/log:ro  # Celui-ci est OK
```
- Conteneurs peuvent modifier les fichiers de configuration
- **Risque de s√©curit√©** : corruption des configs
- Pas de protection contre les modifications accidentelles

### Diagnostic
Les fichiers de configuration mont√©s doivent √™tre en lecture seule.

### Solution
```yaml
logstash:
  volumes:
    - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
    - ./logstash/pipeline:/usr/share/logstash/pipeline:ro

filebeat:
  volumes:
    - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
```

### Impact
- ‚úÖ Protection des configurations
- ‚úÖ S√©curit√© renforc√©e
- ‚úÖ Immuabilit√© des configs

---

## üêõ Bug #12 : Volume logstash_data Manquant

### Sympt√¥mes
```yaml
logstash:
  volumes:
    - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    - ./logstash/pipeline:/usr/share/logstash/pipeline
    # Pas de volume pour /usr/share/logstash/data
```
- Donn√©es Logstash non persistantes (plugins, queues)
- Perte des queues persistentes apr√®s red√©marrage
- R√©installation des plugins √† chaque restart

### Diagnostic
Logstash utilise `/usr/share/logstash/data` pour :
- Persistent queues (PQ)
- Dead letter queues (DLQ)
- Plugins install√©s

### Solution
```yaml
logstash:
  volumes:
    - logstash_data:/usr/share/logstash/data

volumes:
  logstash_data:
    driver: local
```

### Impact
- ‚úÖ Queues persistantes
- ‚úÖ Pas de perte de donn√©es
- ‚úÖ Plugins conserv√©s

---

## üêõ Bug #13 : Volume kibana_data Manquant

### Sympt√¥mes
```yaml
kibana:
  # Pas de volume pour /usr/share/kibana/data
```
- Dashboards et visualisations perdus apr√®s red√©marrage
- Configuration Kibana non persistante
- Saved objects supprim√©s

### Diagnostic
Kibana stocke dans `/usr/share/kibana/data` :
- Dashboards
- Visualizations
- Saved searches
- Index patterns

### Solution
```yaml
kibana:
  volumes:
    - kibana_data:/usr/share/kibana/data

volumes:
  kibana_data:
    driver: local
```

### Impact
- ‚úÖ Dashboards persistants
- ‚úÖ Configuration conserv√©e
- ‚úÖ Pas de r√©initialisation

---

## üêõ Bug #14 : Filebeat user root Manquant

### Sympt√¥mes
```yaml
filebeat:
  image: docker.elastic.co/beats/filebeat:8.11.0
  # Pas de user: root
  volumes:
    - /var/log:/var/log:ro
    - /var/lib/docker/containers:/var/lib/docker/containers:ro
    - /var/run/docker.sock:/var/run/docker.sock:ro
```
- **Erreur** : `Permission denied` sur `/var/log`, `/var/run/docker.sock`
- Filebeat ne peut pas lire les logs syst√®me
- Pas d'acc√®s au Docker socket

### Diagnostic
Filebeat a besoin de privil√®ges root pour acc√©der aux logs syst√®me et au Docker socket.

### Solution
```yaml
filebeat:
  user: root
  volumes:
    - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
    - filebeat_data:/usr/share/filebeat/data
    - /var/lib/docker/containers:/var/lib/docker/containers:ro
    - /var/run/docker.sock:/var/run/docker.sock:ro
  command: filebeat -e -strict.perms=false
```

### Explications
- `user: root` : N√©cessaire pour acc√®s syst√®me
- `-strict.perms=false` : Permet les permissions 644 sur le config
- Volume `filebeat_data` : Persistance du registry

### S√©curit√©
En production, utiliser des capabilities sp√©cifiques plut√¥t que root complet.

### Impact
- ‚úÖ Filebeat peut lire les logs
- ‚úÖ Acc√®s au Docker socket
- ‚úÖ Collecte fonctionnelle

---

## üìä R√©sum√© des Corrections

| Bug | Cat√©gorie | Gravit√© | Impact |
|-----|-----------|---------|--------|
| #1 - Version obsol√®te | Syntaxe | ‚ö†Ô∏è Moyenne | Warnings |
| #2 - Pas de r√©seau | S√©curit√© | üî¥ Haute | Isolation |
| #3 - Health checks absents | Fiabilit√© | üî¥ CRITIQUE | Erreurs d√©marrage |
| #4 - depends_on simple | Fiabilit√© | üî¥ CRITIQUE | Connexions √©chou√©es |
| #5 - Ports hardcod√©s | Configuration | ‚ö†Ô∏è Moyenne | Flexibilit√© |
| #6 - M√©moire hardcod√©e | Performance | ‚ö†Ô∏è Moyenne | Optimisation |
| #7 - Container names | Maintenabilit√© | üü° Basse | Lisibilit√© |
| #8 - Restart policies | Production | üî¥ Haute | Disponibilit√© |
| #9 - ulimits manquants | Performance | üî¥ CRITIQUE | Crash ES |
| #10 - memory_lock absent | Performance | üî¥ Haute | Swap/Latence |
| #11 - Volumes read-only | S√©curit√© | ‚ö†Ô∏è Moyenne | Protection |
| #12 - logstash_data | Persistance | ‚ö†Ô∏è Moyenne | Perte donn√©es |
| #13 - kibana_data | Persistance | ‚ö†Ô∏è Moyenne | Perte dashboards |
| #14 - Filebeat user root | Fonctionnel | üî¥ CRITIQUE | Permission denied |

### Statistiques
- **Total bugs** : 14
- **Critiques** : 4 (health checks, depends_on, ulimits, filebeat user)
- **Hautes** : 3 (r√©seau, restart, memory_lock)
- **Moyennes** : 6
- **Basses** : 1

---

## üéØ Points Cl√©s ELK Stack

### Pour Elasticsearch
- **ulimits obligatoires** (memlock + nofile)
- **bootstrap.memory_lock=true** pour √©viter le swap
- **Health check sur /_cluster/health**
- **M√©moire** : Minimum 1g, recommand√© 2-4g prod
- **Port 9200** : API REST
- **Port 9300** : Communication inter-nodes (pas n√©cessaire en single-node)

### Pour Logstash
- **Volume data obligatoire** pour persistent queues
- **Health check sur :9600/_node/stats**
- **D√©pendance strict d'Elasticsearch**
- **Configurations read-only**
- **Port 5044** : Beats input
- **Port 5000** : TCP/UDP input

### Pour Kibana
- **Volume data pour dashboards**
- **Health check sur :5601/api/status**
- **Start period 90s** (initialisation longue)
- **D√©pendance d'Elasticsearch**

### Pour Filebeat
- **user: root obligatoire**
- **-strict.perms=false** dans la commande
- **Volumes read-only pour configs**
- **Volume data pour registry**
- **D√©pendances : Elasticsearch + Logstash**

---

## üöÄ Validation

Pour valider les corrections :

```bash
cd exercice-4-elk
chmod +x test.sh
./test.sh
```

Le script v√©rifie :
- ‚úÖ Structure des fichiers
- ‚úÖ Syntaxe YAML
- ‚úÖ Variables d'environnement
- ‚úÖ Configuration des 4 services
- ‚úÖ Health checks et ulimits
- ‚úÖ Volumes et persistance
- ‚úÖ Security best practices
- ‚úÖ Tous les bugs corrig√©s

---

**Date d'analyse** : 2024-12-05  
**Niveau de difficult√©** : Avanc√© ‚≠ê‚≠ê‚≠ê‚≠ê  
**Temps de r√©solution estim√©** : 60-90 minutes  
**Stack Version** : Elastic Stack 8.11.0
